remDr$closeServer()
# parse index table
content <- read_html("iea-renewables.html", encoding = "utf8")
tabs <- html_table(content, fill = TRUE)
tab <- tabs[[1]]
# add names
names(tab) <- c("title", "country", "year", "status", "type", "target")
View(tab)
head(tab)
img <- "http://www.r-datacollection.com/img/adcr-cover-wiley4.jpg"
img <- "http://www.r-datacollection.com/img/adcr-cover-wiley4.jpg"
download.file(img, "img.jpg", mode="wb")
## overview
browseURL("http://ropensci.org/")
browseURL("https://github.com/ropensci/opendata")
browseURL("https://cran.r-project.org/web/views/WebTechnologies.html")
# API documentation
browseURL("http://ip-api.com/")
# ipapi package
#devtools::install_github("hrbrmstr/ipapi")
library(ipapi)
geolocate
# function call
ip_df <- geolocate(c(NA, "", "10.0.1.1", "72.33.67.89", "www.nytimes.com", "search.twitter.com"), .progress = TRUE)
View(ip_df)
?geolocate
browseURL("http://developer.nytimes.com/article_search_v2.json#/README")
# overview
browseURL("https://cran.r-project.org/web/packages/rtimes/vignettes/rtimes_vignette.html")
# register for key at
browseURL("http://developer.nytimes.com/")
load("/Users/simonmunzert/rkeys.RDa")
nytimes_apikey
load("/Users/simonmunzert/Munzert Dropbox/Simon Munzert/rkeys.RDa")
nytimes_apikey
Sys.setenv(NYTIMES_AS_KEY = nytimes_apikey)
?as_search
as_search(q = "Bernie Sanders", begin_date = "20180101", end_date = '20180930')
terms <- c("John McCain", "Nancy Pelosi", "Bernie Sanders", "Al Franken", "Marco Rubio", "Paul Ryan", "Elizabeth Warren", "Mitch McConnell", "Tim Kaine", "Dianne Feinstein")
nytimes_hits <- numeric()
for(i in seq_along(terms)) {
nytimes_hits[i] <-  as_search(q = terms[i], begin_date = "20180101", end_date = '20180930')$meta$hits
Sys.sleep(runif(1, 1, 2))
}
nytimes_hits_df <- data.frame(name = terms, nytimes_hits, stringsAsFactors = FALSE)
head(nytimes_hits_df)
# API documentation
browseURL("http://ip-api.com/docs/")
# manual API call, XML data
url <- "http://ip-api.com/xml/"
ip_parsed <- xml2::read_xml(url)
ip_parsed
ip_list <- as_list(ip_parsed)
ip_list
ip_list %>% unlist
ame(stringsAsFactors = FALSE)
ip_list %>% unlist %>% t %>% as.data.frame(stringsAsFactors = FALSE)
dat <-
# manual API call, JSON data
url <- "http://ip-api.com/json"
dat <- ip_list %>% unlist %>% t %>% as.data.frame(stringsAsFactors = FALSE)
head(dat)
names(dat)
names(dat) <- str_replace(names(dat), "query.", "")
names(dat)
dat
# manual API call, JSON data
url <- "http://ip-api.com/json"
ip_parsed <- jsonlite::fromJSON(url)
ip_parsed
as.data.frame(ip_parsed, stringsAsFactors = FALSE)
# modify call
fromJSON("http://ip-api.com/json/72.33.67.89") %>% as.data.frame(stringsAsFactors = FALSE)
fromJSON("http://ip-api.com/json/www.nytimes.com") %>% as.data.frame(stringsAsFactors = FALSE)
# build function
ipapi_grabber <- function(ip = "") {
dat <- fromJSON(paste0("http://ip-api.com/json/", ip)) %>% as.data.frame(stringsAsFactors = FALSE)
dat
}
ipapi_grabber("193.17.243.1")
source("packages.r")
library(pageviews)
ls("package:pageviews")
?article_pageviews
trump_views <- article_pageviews(project = "en.wikipedia", article = "Donald Trump", user_type = "user", start = "2016010100", end = "2017051500")
head(trump_views)
clinton_views <- article_pageviews(project = "en.wikipedia", article = "Hillary Clinton", user_type = "user", start = "2016010100", end = "2017051500")
plot(trump_views$date, trump_views$views, col = "red", type = "l")
lines(clinton_views$date, clinton_views$views, col = "blue")
## api key (example below is not a real key)
load("/Users/simonmunzert/rkeys.RDa") # <--- adapt path here; see above!
## name assigned to created app
appname <- "TwitterToR" # <--- add your Twitter App name here!
## register app
twitter_token <- create_token(
app = appname,
consumer_key = TwitterToR_twitterkey,
consumer_secret = TwitterToR_twittersecret,
access_token = TwitterToR_accesstoken,
access_secret = TwitterToR_accesssecret,
set_renv = FALSE)
## check if everything worked
rt <- search_tweets("merkel", n = 200, token = twitter_token)
View(rt)
# some advice for search:
browseURL("https://developer.twitter.com/en/docs/tweets/search/guides/standard-operators")
trump_bad <- search_tweets(URLencode("trump :("), n = 100, include_rts = FALSE, lang = "en", token = twitter_token)
trump_bad$text
trump_good <- search_tweets(URLencode("trump :)"), n = 100, include_rts = FALSE, lang = "en", token = twitter_token)
trump_good
trump_good$text
trump_images <- search_tweets(URLencode("trump filter:images"), n = 100, include_rts = FALSE, lang = "en", token = twitter_token)
trump_images
trump_images$text
# check rate limits
rate_limits(token = twitter_token) %>% View()
# set keywords used to filter tweets
q <- paste0("merkel,trump,macron")
# parse directly into data frame
twitter_stream <- stream_tweets(q = q, timeout = 30, token = twitter_token)
class(twitter_stream)
View(twitter_stream)
user_df <- lookup_users("RDataCollection")
names(user_df)
user_timeline_df <- get_timeline("RDataCollection")
names(user_timeline_df)
user_favorites_df <- get_favorites("RDataCollection")
names(user_favorites_df)
followers <- get_followers("RDataCollection")
friends <- get_friends("RDataCollection")
source("packages.r")
# add header fields with rvest + httr
url <- "http://spiegel.de/schlagzeilen"
library(robotstxt)
paths_allowed("/", "http://google.com/", bot = "*")
paths_allowed("/", "https://facebook.com/", bot = "*")
paths_allowed("/imgres", "http://google.com/", bot = "*")
paths_allowed("/imgres", "http://google.com/", bot = "Twitterbot")
r_text <- get_robotstxt("https://www.google.com/")
r_parsed <- parse_robotstxt(r_text)
names(r_parsed)
View(r_parsed)
table(r_parsed$permissions$useragent, r_parsed$permissions$field)
browseURL("https://cran.r-project.org/web/packages/taskscheduleR/vignettes/taskscheduleR.html")
browseURL("https://developer.apple.com/library/content/documentation/MacOSX/Conceptual/BPSystemStartup/Chapters/ScheduledJobs.html")
# 1. create script (e.g., "spiegel_scraper.R")
# 2. add "#!/usr/local/bin/Rscript" to the top of the script
# 3. create plist file
# 4. load plist file into launchd scheduler and start it (via Terminal):
system("launchctl load ~/Library/LaunchAgents/spiegelheadlines.plist")
system("launchctl start spiegelheadlines")
system("launchctl list")
# 5. stop and unload it when desired
system("launchctl stop spiegelheadlines")
system("launchctl unload ~/Library/LaunchAgents/spiegelheadlines.plist")
load("/Users/simonmunzert/GitHub/rscraping-washington-2018/code/news_scraper/nytimes_df_2018_june.RDa")
View(nytimes_df)
# an example string
small.frogs <- "Små grodorna, små grodorna är lustiga att se."
small.frogs
# check encoding
Encoding(small.frogs)
# declare (wrong) encoding - what happens?
Encoding(small.frogs) <- "latin1"
small.frogs
# declare (right) encoding again
Encoding(small.frogs) <- "utf8"
small.frogs # in this case, we could restore the original string
## preparations -----------------------
source("packages.r")
## inspect the source code in your browser ---------------
browseURL("http://www.biermap24.de/brauereiliste.php")
# set temporary working directory
tempwd <- ("../data/breweriesGermany")
dir.create(tempwd)
setwd(tempwd)
## step 1: fetch list of cities with breweries
url <- "http://www.biermap24.de/brauereiliste.php"
content <- read_html(url)
anchors <- html_nodes(content, xpath = "//tr/td[2]")
cities <- html_text(anchors)
cities
cities <- str_trim(cities)
cities <- cities[str_detect(cities, "^[[:upper:]]+.")]
cities <- cities[6:length(cities)]
length(cities)
length(unique(cities))
sort(table(cities))
## step 2: geocode cities
# geocoding takes a while -> save results in local cache file
# 2500 requests allowed per day
if ( !file.exists("breweries_geo.RData")){
pos <- geocode(cities)
geocodeQueryCheck()
save(pos, file="breweries_geo.RData")
} else {
load("breweries_geo.RData")
}
head(pos)
## step 3: plot breweries of Germany
brewery_map <- get_map(location=c(lon = mean(c(min(pos$lon), max(pos$lon))), lat = mean(c(min(pos$lat), max(pos$lat)))), zoom=6, maptype="terrain", source = "stamen")
# add background raster graphic
library(OpenStreetMap)
library(rgdal)
map <- openmap(upperLeft = c(55.5, 5), lowerRight = c(46, 16), type = "osm")
pos_mercator <- projectMercator(pos$lat, pos$lon) %>% as.data.frame
autoplot(map) + geom_point(aes(x, y), data=pos_mercator, size = .5, color = "red")
## return to base working drive
setwd("../../code")
source("packages.r")
## string matching ----------
# example
raw.data <- "555-1239Moe Szyslak(636) 555-0113Burns, C. Montgomery555-6542Rev. Timothy Lovejoy555 8904Ned Flanders636-555-3226Simpson, Homer5553642Dr. Julius Hibbert"
name <- unlist(str_extract_all(raw.data, "[[:alpha:]., ]{2,}"))
name
phone <- unlist(str_extract_all(raw.data, "\\(?(\\d{3})?\\)?(-| )?\\d{3}(-| )?\\d{4}"))
phone
data.frame(name = name, phone = phone)
# running example
example.obj <- "1. A small sentence. - 2. Another tiny sentence."
# self match
str_extract(example.obj, "small")
str_extract(example.obj, "banana")
str_extract_all(example.obj, "e")
# multiple matches
(out <- str_extract_all(c("text", "manipulation", "basics"), "a"))
# case sensitivity
str_extract(example.obj, "small")
str_extract(example.obj, "SMALL")
str_extract(example.obj, ignore.case("SMALL")) # wrong
str_extract(example.obj, regex("SMALL", ignore_case = TRUE))
# match empty space
str_extract(example.obj, "mall sent")
# match the beginning of a string
str_extract(example.obj, "^1")
str_extract(example.obj, "^2")
# match the end of a string
str_extract(example.obj, "sentence$")
str_extract(example.obj, "sentence.$")
# pipe operator
unlist(str_extract_all(example.obj, "tiny|sentence"))
# wildcard
str_extract(example.obj, "sm.ll")
# character class
str_extract(example.obj, "sm[abc]ll")
# character class: range
str_extract(example.obj, "sm[a-p]ll")
# character class: additional characters
unlist(str_extract_all(example.obj, "[uvw. ]"))
# pre-defined character classes
unlist(str_extract_all(example.obj, "[:punct:]"))
unlist(str_extract_all(example.obj, "[[:punct:]ABC]"))
unlist(str_extract_all(example.obj, "[^[:alnum:]]"))
# additional shortcuts
unlist(str_extract_all(example.obj, "\\w+"))
# word edges
unlist(str_extract_all(example.obj, "e\\b"))
unlist(str_extract_all(example.obj, "e\\B"))
source("packages.r")
## parse HTML --------------------
# parse with read_html
parsed_doc <- read_html("https://www.facebook.com")
parsed_doc
## basic XPath grammar -----------------
# import running example
parsed_doc <- read_html("http://www.r-datacollection.com/materials/ch-4-xpath/fortunes/fortunes.html")
parsed_doc
# absolute paths
html_nodes(parsed_doc, xpath = "/html/body/div/p/i")
# relative paths
html_nodes(parsed_doc, xpath = "//body//p/i")
html_nodes(parsed_doc, xpath = "//p/i")
html_nodes(parsed_doc, xpath = "//i")
# wildcard (for ONE node)
html_nodes(parsed_doc, xpath = "/html/body/div/*/i")
html_nodes(parsed_doc, xpath = "/html/body/*/i") # does not work
# ancestor
html_nodes(parsed_doc, xpath = "//a/ancestor::div")
html_nodes(parsed_doc, xpath = "//a/ancestor::div//i")
# sibling
html_nodes(parsed_doc, xpath = "//p/preceding-sibling::h1")
# Parent
html_nodes(parsed_doc, xpath = "//title/parent::*")
# numeric
html_nodes(parsed_doc, xpath = "//div/p[1]")
html_nodes(parsed_doc, xpath =  "//div/p[last()]")
html_nodes(parsed_doc, xpath = "//div[count(.//a)>0]")
html_nodes(parsed_doc, xpath = "//div[count(./@*)>2]")
html_nodes(parsed_doc, xpath = "//*[string-length(text())>50]")
# text-based
html_nodes(parsed_doc, xpath = "//div[@date='October/2011']")
html_nodes(parsed_doc, xpath = "//*[contains(text(), 'magic')]")
html_nodes(parsed_doc, xpath = "//div[starts-with(./@id, 'R')]")
html_nodes(parsed_doc, xpath = "//div[substring-after(./@date, '/')='2003']//i")
# values
html_nodes(parsed_doc, xpath = "//title") %>% html_text()
# values
html_nodes(parsed_doc, xpath = "//title") %>% html_text()
# attributes
html_nodes(parsed_doc, xpath = "//div") %>% html_attrs() # all attributes
html_nodes(parsed_doc, xpath = "//div") %>% html_attr("lang") # single attribute
source("packages.r")
## basic workflow of scraping with rvest  ----------
# see also: https://github.com/hadley/rvest
# convenient package to scrape information from web pages
# builds on other packages, such as xml2 and httr
# provides very intuitive functions to import and process webpages
# 1. specify URL
url <- "https://en.wikipedia.org/wiki/List_of_tallest_buildings_in_Washington,_D.C."
# 1. specify URL
url <- "https://en.wikipedia.org/wiki/List_of_tallest_buildings_in_Germany"
browseURL(url)
# 2. download static HTML behind the URL and parse it
url_parsed <- read_html(url)
# 3. extract specific nodes with XPath
nodes <- html_nodes(url_parsed, xpath = '//td[2]/a')
# 4. extract content from nodes
article_links <- html_text(nodes)
article_links[article_links != ""]
head(article_links)
length(article_links)
## extract data from tables --------------
## HTML tables
# ... are a special case for scraping because they are already very close to the data structure you want to build up in R
# ... come with standard tags and are usually easily identifiable
## scraping HTML tables with rvest
url_p <- read_html("https://en.wikipedia.org/wiki/List_of_MPs_elected_in_the_United_Kingdom_general_election,_1992")
tables <- html_table(url_p, header = TRUE, fill = TRUE)
mps <- tables[[4]]
head(mps)
names(mps) <- c("constituency", "mp", "party")
mps <- mps[2:nrow(mps),]
mps <- filter(mps, !str_detect(constituency, fixed("[edit]")))
table(mps$party, str_detect(mps$mp, "^Sir ")) # how many "Sirs" per party?
url <- "https://www.washingtonpost.com"
url_parsed <- read_html(url)
xpath <- '//*[contains(concat( " ", @class, " " ), concat( " ", "text-align-inherit", " " ))] | //*[(@id = "f0HwxnQWxXu66r")]//*[contains(concat( " ", @class, " " ), concat( " ", "equalize-height-target", " " ))]'
headings_nodes <- html_nodes(url_parsed, xpath = xpath)
headings_nodes
headings <- html_text(headings_nodes)
headings <- str_subset(headings, "^[:alnum:]")
head(headings)
headings_nodes <- html_nodes(url_parsed, xpath = '//*[(@id = "main-content")]//*[contains(concat( " ", @class, " " ), concat( " ", "text-align-inherit", " " ))]')
headings <- html_text(headings_nodes)
headings <- str_subset(headings, "^[:alnum:]")
head(headings)
length(headings)
source("packages.r")
# load RSelenium
library(RSelenium)
# install current version of Java SE Runtime Environment
browseURL("http://www.oracle.com/technetwork/java/javase/downloads/jre8-downloads-2133155.html")
# initiate Selenium driver
rD <- rsDriver()
# initiate Selenium driver
rD <- rsDriver()
# start browser, navigate to page
url <- "http://www.iea.org/policiesandmeasures/renewableenergy/"
remDr$navigate(url)
remDr <- rD[["client"]]
# start browser, navigate to page
url <- "http://www.iea.org/policiesandmeasures/renewableenergy/"
remDr$navigate(url)
# open regions menu
xpath <- '//*[@id="main"]/div/form/div[1]/ul/li[1]/span'
regionsElem <- remDr$findElement(using = 'xpath', value = xpath)
regionsElem$clickElement() # click on button
# selection "European Union"
xpath <- '//*[@id="main"]/div/form/div[1]/ul/li[1]/ul/li[5]/label'
euElem <- remDr$findElement(using = 'xpath', value = xpath)
euElem$clickElement() # click on button
# set time frame
xpath <- '//*[@id="main"]/div/form/div[5]/select[1]'
fromDrop <- remDr$findElement(using = 'xpath', value = xpath)
clickFrom <- fromDrop$clickElement() # click on drop-down menu
writeFrom <- fromDrop$sendKeysToElement(list("2000")) # enter start year
xpath <- '//*[@id="main"]/div/form/div[5]/select[2]'
toDrop <- remDr$findElement(using = 'xpath', value = xpath)
clickTo <- toDrop$clickElement() # click on drop-down menu
writeTo <- toDrop$sendKeysToElement(list("2010")) # enter end year
# click on search button
xpath <- '//*[@id="main"]/div/form/button[2]'
searchElem <- remDr$findElement(using = 'xpath', value = xpath)
resultsPage <- searchElem$clickElement() # click on button
# store index page
output <- remDr$getPageSource(header = TRUE)
write(output[[1]], file = "iea-renewables.html")
# close connection
remDr$closeServer()
# parse index table
content <- read_html("iea-renewables.html", encoding = "utf8")
tabs <- html_table(content, fill = TRUE)
tab <- tabs[[1]]
# add names
names(tab) <- c("title", "country", "year", "status", "type", "target")
head(tab)
source("packages.r")
# ipapi package
#devtools::install_github("hrbrmstr/ipapi")
library(ipapi)
# function call
ip_df <- geolocate(c(NA, "", "10.0.1.1", "72.33.67.89", "www.nytimes.com", "search.twitter.com"), .progress = TRUE)
View(ip_df)
# overview
browseURL("https://cran.r-project.org/web/packages/rtimes/vignettes/rtimes_vignette.html")
load("/Users/simonmunzert/Munzert Dropbox/Simon Munzert/rkeys.RDa")
Sys.setenv(NYTIMES_AS_KEY = nytimes_apikey)
terms <- c("John McCain", "Nancy Pelosi", "Bernie Sanders", "Al Franken", "Marco Rubio", "Paul Ryan", "Elizabeth Warren", "Mitch McConnell", "Tim Kaine", "Dianne Feinstein")
## peparations -------------------
source("packages.r")
## name assigned to created app
appname <- "TwitterToR" # <--- add your Twitter App name here!
## api key (example below is not a real key)
load("/Users/simonmunzert/rkeys.RDa") # <--- adapt path here; see above!
## register app
twitter_token <- create_token(
app = appname,
consumer_key = TwitterToR_twitterkey,
consumer_secret = TwitterToR_twittersecret,
access_token = TwitterToR_accesstoken,
access_secret = TwitterToR_accesssecret,
set_renv = FALSE)
load("/Users/simonmunzert/twitter_token.rds") # <--- adapt path here; see above!
TwitterToR_twitterkey <- "uBoA6wJPiWXrcuTWUFbaTIApT"  # <--- add your Twitter key here!
TwitterToR_twittersecret <- "myhHWBOP3bPzd4oa38I9H5SesmAuUp4YZzgv97DtDxi4nlYlQM" # <--- add your Twitter secret here!
TwitterToR_accesstoken <- "124133966-6Nd4d3fcemYA4nb2qcXJWvOJZlAij8UAoV9GJCtD"  # <--- add your Twitter access token here!
TwitterToR_accesssecret <- "01GNhWMjwVIkaHEEkhQIxIoZQDLQXLe23cgpPV2FOmFjE" # <--- add your Twitter secret here!
TwitterToR_twitterkey <- "uBoA6wJPiWXrcuTWUFbaTIApT"  # <--- add your Twitter key here!
TwitterToR_twittersecret <- "myhHWBOP3bPzd4oa38I9H5SesmAuUp4YZzgv97DtDxi4nlYlQM" # <--- add your Twitter secret here!
TwitterToR_accesstoken <- "124133966-6Nd4d3fcemYA4nb2qcXJWvOJZlAij8UAoV9GJCtD"  # <--- add your Twitter access token here!
TwitterToR_accesssecret <- "01GNhWMjwVIkaHEEkhQIxIoZQDLQXLe23cgpPV2FOmFjE" # <--- add your Twitter secret here!
save(TwitterToR_twitterkey,
TwitterToR_twittersecret,
TwitterToR_accesstoken,
TwitterToR_accesssecret,
file = paste0(normalizePath("~/"),"/rkeys2.RDa")) # <--- this is where your keys are locally stored!
source("packages.r")
## api key (example below is not a real key)
load("/Users/simonmunzert/rkeys2.RDa") # <--- adapt path here; see above!
TwitterToR_twitterkey <- "uBoA6wJPiWXrcuTWUFbaTIApT"  # <--- add your Twitter key here!
TwitterToR_twittersecret <- "myhHWBOP3bPzd4oa38I9H5SesmAuUp4YZzgv97DtDxi4nlYlQM" # <--- add your Twitter secret here!
TwitterToR_accesstoken <- "124133966-6Nd4d3fcemYA4nb2qcXJWvOJZlAij8UAoV9GJCtD"  # <--- add your Twitter access token here!
TwitterToR_accesssecret <- "01GNhWMjwVIkaHEEkhQIxIoZQDLQXLe23cgpPV2FOmFjE" # <--- add your Twitter secret here!
save(TwitterToR_twitterkey,
TwitterToR_twittersecret,
TwitterToR_accesstoken,
TwitterToR_accesssecret,
file = paste0(normalizePath("~/"),"/twitterkeys.RDa")) # <--- this is where your keys are locally stored!
## name assigned to created app
appname <- "TwitterToR" # <--- add your Twitter App name here!
## register app
twitter_token <- create_token(
app = appname,
consumer_key = TwitterToR_twitterkey,
consumer_secret = TwitterToR_twittersecret,
access_token = TwitterToR_accesstoken,
access_secret = TwitterToR_accesssecret,
set_renv = FALSE)
## check if everything worked
rt <- search_tweets("merkel", n = 200, token = twitter_token)
View(rt)
source("packages.r")
# inspect form
session <- html_session("http://www.google.com")
url_parsed <- read_html("http://www.google.com")
search <- html_form(session)[[1]]
search
# set form parameters
form <- set_values(search, q = "kneipen kreuzberg")
# submit form
google_search <- submit_form(session, form)
# retrieve results
url_parsed <- read_html(google_search)
hits_text <- html_nodes(url_parsed, xpath = "//*[@class='r']//a") %>% html_text()
hits_links <- html_nodes(url_parsed, xpath = "//*[@class='r']//a") %>% html_attr("href")
hits_text
hits_links
## another example: WordNet Search
# inspect webpage
url <- "http://wordnetweb.princeton.edu/perl/webwn"
browseURL(url)
url_parsed <- read_html(url)
html_form(url_parsed)
wordnet <- html_form(url_parsed)[[1]]
wordnet
# test it online with "data"; alternative approach using html_session()
browseURL("http://www.whoishostingthis.com/tools/user-agent/")
session <- html_session(url, user_agent(uastring))
uastring <- "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36"
session <- html_session(url, user_agent(uastring))
wordnet_form <- set_values(wordnet, s = "data")
wordnet_search <- submit_form(session, wordnet_form)
url_parsed <- read_html(wordnet_search)
url_parsed %>% html_nodes(xpath = "//li") %>% html_text()
session <- html_session(url, user_agent(uastring))
wordnet_form <- set_values(wordnet, s = "data", o2 = "1")
wordnet_search <- submit_form(session, wordnet_form)
url_parsed <- read_html(wordnet_search)
url_parsed %>% html_nodes(xpath = "//li") %>% html_text()
## POST forms
# goal: gathering data from read-able at http://read-able.com/
url <- "http://read-able.com/"
browseURL(url)
url_parsed <- read_html(url)
html_form(url_parsed)
readable <- html_form(url_parsed)[[2]]
sentence <- '"It is a capital mistake to theorize before one has data. Insensibly one begins to twist facts to suit theories, instead of theories to suit facts." - Arthur Conan Doyle, Sherlock Holmes'
readable_form <- set_values(readable, directInput = sentence)
session <- html_session(url, user_agent(uastring))
readable_search <- submit_form(session, readable_form)
url_parsed <- read_html(readable_search)
html_table(url_parsed)
